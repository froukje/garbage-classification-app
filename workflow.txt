# start service
bentoml serve service.py:service --reload

# make prediction locally
#curl -F media=@test.jpg "http://127.0.0.1:3000/predict" - this does only work if input is image
python3 test_predict_endpoint.py

# create bentofile.yaml wih needed packages for the service. This controls the bento.

# build the bento
bentoml build

# see all bentos
bentoml list

# build docker image
bentoml containerize garbage_classification_service:hc64svd7422siwew

# run docker image
docker run -it --rm -p 3000:3000 garbage_classification_service:4cqoeft75gr4owew


#####
Deploy on AWS

# login to AWS and create an Elastic Container Registry (with default values)
# On the top is a button "push options"

# login to AWS
aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 697020065496.dkr.ecr.eu-west-1.amazonaws.com

# tag the image
docker tag garbage_classification_service:4cqoeft75gr4owew 697020065496.dkr.ecr.eu-west-1.amazonaws.com/garbage-classifier-repo:latest

# push the image
docker push 697020065496.dkr.ecr.eu-west-1.amazonaws.com/garbage-classifier-repo:latest

# ECR is a place to put the image, the place to run it is Elastic Container Service
# Create a Cluster (choose a name, the rest as default)
# Create a Task (default (Fargate), operating system: Linux)
# Add Container, add image URI from uploaded image from ECR
# Add port 3000

#####
Streamlit app

# start app
# streamlit only works with pipenv not with conda!
pipenv install
pipenv shell
streamlit run streamlit_app.py
# app is running on: localhost:8501

####
docker-compose
# Dockerfile for streamlit app
docker build -t streamlit_app:v1 .

####
kubernetes

####
heroku

